# awesome-automl   
collecting related resources of automated machine learning here. some links were from below,keyword:"automl,autodl"
- [][hibayesian/awesome-automl-papers](https://github.com/hibayesian/awesome-automl-papers)   
- [x][literature-on-neural-architecture-search](http://www.ml4aad.org/literature-on-neural-architecture-search/)
- [x][Algorithm Configuration Literature](http://aclib.net/acbib/)
- [][windmaple/awesome-AutoML](https://github.com/windmaple/awesome-AutoML)
- [][DataSystemsGroupUT/AutoML_Survey](https://github.com/DataSystemsGroupUT/AutoML_Survey)
- [][D-X-Y/Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)
- [][D-X-Y/AutoDL-Projects](https://github.com/D-X-Y/AutoDL-Projects)
- [][HuaizhengZhang/Awesome-System-for-Machine-Learning](https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning)
- [][dragen1860/awesome-AutoML](https://github.com/dragen1860/awesome-AutoML)
- [][lihanghang/Deep-learning-And-Paper](https://github.com/lihanghang/Deep-learning-And-Paper)
- [][anonymone/Neural-Architecture-Search](https://github.com/anonymone/Neural-Architecture-Search)
- [][ssheikholeslami/automl-resources](https://github.com/ssheikholeslami/automl-resources)
- [][guan-yuan/awesome-AutoML-and-Lightweight-Models](https://github.com/guan-yuan/awesome-AutoML-and-Lightweight-Models)
- [][JaimeTang/AutoML](https://github.com/JaimeTang/AutoML)
- [][Yipeng-Sun/AutoML-NAS-papers](https://github.com/Yipeng-Sun/AutoML-NAS-papers)
- [][Yejining/Survey](https://github.com/Yejining/Survey)
- [][lidderupk/automl-intro](https://github.com/lidderupk/automl-intro)
- [][oskar-j/awesome-auto-ml](https://github.com/oskar-j/awesome-auto-ml)
- [][BruceQFWang/Meta-learning-Paper-List](https://github.com/BruceQFWang/Meta-learning-Paper-List)
- [][YIWEI-CHEN/awesome-automated-machine-learning](https://github.com/YIWEI-CHEN/awesome-automated-machine-learning)
- [][pbiecek/automl_resources](https://github.com/pbiecek/automl_resources)
- [][jphall663/automl_resources](https://github.com/jphall663/automl_resources)
- [][LevineHuang/AutoML-Tutorials](https://github.com/LevineHuang/AutoML-Tutorials)
- [][eug/ai-resources](https://github.com/eug/ai-resources)

--------------------------------
keyword:"meta learning"
- [][floodsung/Meta-Learning-Papers](https://github.com/floodsung/Meta-Learning-Papers)
- [][sudharsan13296/Awesome-Meta-Learning](https://github.com/sudharsan13296/Awesome-Meta-Learning)
- [][dragen1860/awesome-meta-learning](https://github.com/dragen1860/awesome-meta-learning)
- [][oneHuster/Meta-Learning-Papers](https://github.com/oneHuster/Meta-Learning-Papers)
- [][csyanbin/Few-shot-Meta-learning-papers](https://github.com/csyanbin/Few-shot-Meta-learning-papers)
- [][ha-lins/MetaLearning4NLP-Papers](https://github.com/ha-lins/MetaLearning4NLP-Papers)
- [][johnnyasd12/awesome-few-shot-meta-learning](https://github.com/johnnyasd12/awesome-few-shot-meta-learning)
- [][jarvisWang0903/Meta-Learning-PaperReading](https://github.com/jarvisWang0903/Meta-Learning-PaperReading)
- [][Deepest-Project/meta-learning-study](https://github.com/Deepest-Project/meta-learning-study)
- [][metarl/awesome-metarl](https://github.com/metarl/awesome-metarl)
- [][BruceQFWang/Meta-learning-Paper-List](https://github.com/BruceQFWang/Meta-learning-Paper-List)
- [][rootlu/MetaLearningPapers](https://github.com/rootlu/MetaLearningPapers)
- [][Meta-Learning/Awesome-Meta-Learning](https://github.com/Meta-Learning/Awesome-Meta-Learning)
- [][anthonysicilia/MetaLearningPapers](https://github.com/anthonysicilia/MetaLearningPapers)
- [][dragen1860/Meta-Learning-Papers)](https://github.com/dragen1860/Meta-Learning-Papers)
- [][sorrowyn/awesome-metarl-2](https://github.com/sorrowyn/awesome-metarl-2)
- [][Alro10/meta-learning-resources](https://github.com/Alro10/meta-learning-resources)
- [][clxiao/Meta-Learning-Papers](https://github.com/clxiao/Meta-Learning-Papers)
- [][adityagupte95/Meta-Learning-Papers](https://github.com/adityagupte95/Meta-Learning-Papers)

you can take part in [automl Challenge](http://automl.chalearn.org/),[autodl Challenge](https://autodl.chalearn.org/)    
&nbsp;&nbsp;  or find competitions in [kaggle](https://www.kaggle.com)    
&nbsp;&nbsp;  or get search result from [reddit](https://www.reddit.com/search?q=automl), [bing](https://cn.bing.com/search?q=automated+machine+learning&FORM=BESBTB&ensearch=1), [quora](https://www.quora.com/search?q=automl)(search keyword should be "automatic machine learning","automl","meta learning","automated machine learning" and so on),     
&nbsp;&nbsp;  or access the website [automl](http://www.ml4aad.org),   
&nbsp;&nbsp;  or search your keyword in [arxiv papers info](https://github.com/ChanChiChoi/tiny-crawler/tree/master/paperMeta4arxiv),    
&nbsp;&nbsp;  or others to find some perfect resources there.

---
This papers or books or slides are ordered by years, before each entity is the theme the entity belonged, if you want to choice one theme, e.g. "Architecture Search", you can **_ctrl+F_** then highlight the papers.   
Themes are as follow:   
- 1.【Architecture Search】:    
【Random Search】; 【Evolutionary Algorithms】;【Transfer Learning】;【Reinforcement Learning】;【Local Search】; 
- 2.【Hyperparameter Optimization】:    
【Bayesian Optimization】;【Meta Learning】;【Particle Swarm Optimization】;【Lipschitz Functions】;【Random Search】;【Transfer Learning】;【Local Search】;        
- 3.【Multi-Objective NAS】;  
- 4.【Automated Feature Engineering】;【Reinforcement Learning】;【Meta Learning】;        
- 5.【Frameworks】;
- 6.【Meta Learning】;    
- 7.【Miscellaneous】  

ps:The theme is a bit confusing and I will modify it later.

---
## Papers

#### 1990
- 【Architecture Search】Fahlman, Scott E and Lebiere, Christian. [The cascade correlation learning architecture](http://papers.nips.cc/paper/207-the-cascade-correlation-learning-architecture.pdf). In NIPS, pp. 524–532,1990.

#### 2002
- 【Architecture Search】【Evolutionary Algorithms】Stanley K O, Miikkulainen R. [Evolving neural networks through augmenting topologies](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.5457&rep=rep1&type=pdf)[J]. Evolutionary computation, 2002, 10(2): 99-127.

#### 2008
- 【Tutorials】【Meta Learning】[Metalearning - A Tutorial](https://pdfs.semanticscholar.org/5794/1a4891f673cadf06fba02419372aad85c3bb.pdf)
- 【Hyperparameter Optimization】【Particle Swarm Optimization】Lin S W, Ying K C, Chen S C, et al. [Particle swarm optimization for parameter determination and feature selection of support vector machines](http://www.sciencedirect.com/science/article/pii/S0957417407003752)[J]. Expert systems with applications, 2008, 35(4): 1817-1824.
- 【Hyperparameter Optimization】【Meta Learning】Smith-Miles K A. [Cross-disciplinary perspectives on meta-learning for algorithm selection](https://dl.acm.org/citation.cfm?id=1456656)[J]. ACM Computing Surveys (CSUR), 2009, 41(1): 6.
- 【Architecture Search】【Evolutionary Algorithms】Floreano, Dario, D¨urr, Peter, and Mattiussi, Claudio. [Neuroevolution:from architectures to learning](https://infoscience.epfl.ch/record/112676/files/FloreanoDuerrMattiussi2008.pdf). Evolutionary Intelligence, 1(1):47–62, 2008

#### 2009 
- 【Hyperparameter Optimization】【Local Search】Hutter F, Hoos H H, Leyton-Brown K, et al. [ParamILS: an automatic algorithm configuration framework](https://arxiv.org/pdf/1401.3492.pdf)[J]. Journal of Artificial Intelligence Research, 2009, 36: 267-306.
- 【Architecture Search】【Evolutionary Algorithms】Stanley, Kenneth O, D’Ambrosio, David B, and Gauci, Jason. [A hypercube-based encoding for evolving large-scale neural networks](http://axon.cs.byu.edu/Dan/778/papers/NeuroEvolution/stanley3**.pdf). Artificial life, 15(2):185–212, 2009

#### 2010
- 【Bayesian Optimization】Brochu E, Cora V M, De Freitas N. [A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning](https://arxiv.org/pdf/1012.2599v1.pdf)[J]. arXiv preprint arXiv:1012.2599, 2010.
- 【Automated Feature Engineering】【Reinforcement Learning】Gaudel R, Sebag M. [Feature selection as a one-player game](https://hal.inria.fr/docs/00/48/40/49/PDF/fuse_icml10.pdf)[C]//International Conference on Machine Learning. 2010: 359--366.

#### 2011
- 【Hyperparameter Optimization】【Random Search】Bergstra J S, Bardenet R, Bengio Y, et al. [Algorithms for hyper-parameter optimization](https://dl.acm.org/citation.cfm?id=2986743)[C]//Advances in neural information processing systems. 2011: 2546-2554.
- 【Hyperparameter Optimization】【Bayesian Optimization】Hutter F, Hoos H H, Leyton-Brown K. [Sequential model-based optimization for general algorithm configuration](https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf)[C]//International Conference on Learning and Intelligent Optimization. Springer, Berlin, Heidelberg, 2011: 507-523.

#### 2012
- 【Architecture Search】Snoek J, Larochelle H, Adams R P. [Practical bayesian optimization of machine learning algorithms](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)[C]//Advances in neural information processing systems. 2012: 2951-2959.
- 【Hyperparameter Optimization】【Random Search】Bergstra J, Bengio Y. [Random search for hyper-parameter optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305.
- 【Hyperparameter Optimization】【Bayesian Optimization】Snoek J, Larochelle H, Adams R P. [Practical bayesian optimization of machine learning algorithms](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)[C]//Advances in neural information processing systems. 2012: 2951-2959.

#### 2013
- 【Hyperparameter Optimization】【Transfer Learning】Bardenet R, Brendel M, Kégl B, et al. [Collaborative hyperparameter tuning](http://proceedings.mlr.press/v28/bardenet13.pdf)[C]//International Conference on Machine Learning. 2013: 199-207.
- 【Hyperparameter Optimization】【Bayesian Optimization】Bergstra J, Yamins D, Cox D D. [Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures](http://proceedings.mlr.press/v28/bergstra13.pdf)[J]. 2013.
- 【Hyperparameter Optimization】【Bayesian Optimization】Thornton C, Hutter F, Hoos H H, et al. [Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms](http://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/autoweka.pdf)[C]//Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2013: 847-855.
- 【Hyperparameter Optimization】James Bergstra, David D. Cox. [Hyperparameter Optimization and Boosting for Classifying Facial Expressions: How good can a "Null" Model be?](https://arxiv.org/abs/1306.3476)[J]. arXiv preprint arXiv:1306.3476, 2013.

#### 2014
- 【Hyperparameter Optimization】【Transfer Learning】Yogatama D, Mann G. [Efficient transfer learning method for automatic hyperparameter tuning](https://pdfs.semanticscholar.org/75f2/6734972ebaffc6b43d45abd3048ef75f15a5.pdf)[C]//Artificial Intelligence and Statistics. 2014: 1077-1085.

#### 2015
- 【Hyperparameter Optimization】Dougal Maclaurin, David Duvenaud, Ryan P. Adams. [Gradient-based Hyperparameter Optimization through Reversible Learning](https://arxiv.org/abs/1502.03492)[J]. arXiv preprint arXiv:1502.03492, 2015.
- 【Hyperparameter Optimization】Kevin Jamieson, Ameet Talwalkar. [Non-stochastic Best Arm Identification and Hyperparameter Optimization](https://arxiv.org/abs/1502.07943)[J]. arXiv preprint arXiv:1502.07943, 2015.
- 【Architecture Search】【Evolutionary Algorithms】Young S R, Rose D C, Karnowski T P, et al. [Optimizing deep learning hyper-parameters through an evolutionary algorithm](https://www.researchgate.net/profile/Steven_Young11/publication/301463804_Optimizing_deep_learning_hyper-parameters_through_an_evolutionary_algorithm/links/57ac9b7c08ae3765c3bac448/Optimizing-deep-learning-hyper-parameters-through-an-evolutionary-algorithm.pdf)[C]//Proceedings of the Workshop on Machine Learning in High-Performance Computing Environments. ACM, 2015: 4.
- 【Hyperparameter Optimization】【Bayesian Optimization】Wistuba M, Schilling N, Schmidt-Thieme L. [Sequential model-free hyperparameter tuning](http://ieeexplore.ieee.org/abstract/document/7373431/)[C]//Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015: 1033-1038.
- 【Hyperparameter Optimization】【Bayesian Optimization】Snoek J, Rippel O, Swersky K, et al. [Scalable bayesian optimization using deep neural networks](https://dl.acm.org/citation.cfm?id=3045349)[C]//International conference on machine learning. 2015: 2171-2180.
- 【Hyperparameter Optimization】【Bayesian Optimization】Wistuba M, Schilling N, Schmidt-Thieme L. [Learning hyperparameter optimization initializations](http://ieeexplore.ieee.org/abstract/document/7344817/)[C]//Data Science and Advanced Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on. IEEE, 2015: 1-10.
- 【Hyperparameter Optimization】【Bayesian Optimization】Schilling N, Wistuba M, Drumond L, et al. [Joint model choice and hyperparameter optimization with factorized multilayer perceptrons](http://ieeexplore.ieee.org/abstract/document/7372120/)[C]//Tools with Artificial Intelligence (ICTAI), 2015 IEEE 27th International Conference on. IEEE, 2015: 72-79.
- 【Hyperparameter Optimization】【Bayesian Optimization】Wistuba M, Schilling N, Schmidt-Thieme L. [Hyperparameter search space pruning–a new component for sequential model-based hyperparameter optimization](https://dl.acm.org/citation.cfm?id=2991491)[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2015: 104-119.
- 【Hyperparameter Optimization】【Bayesian Optimization】Schilling N, Wistuba M, Drumond L, et al. [Hyperparameter optimization with factorized multilayer perceptrons](https://link.springer.com/chapter/10.1007/978-3-319-23525-7_6)[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2015: 87-103.
- 【Hyperparameter Optimization】【Bayesian Optimization】【more efficient】Feurer M, Klein A, Eggensperger K, et al. [Efficient and robust automated machine learning](https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf)[C]//Advances in Neural Information Processing Systems. 2015: 2962-2970.
- 【Frameworks】Thakur A, Krohn-Grimberghe A.[AutoCompete: A Framework for Machine Learning Competition](https://arxiv.org/pdf/1507.02188.pdf)[J]. arXiv preprint arXiv:1507.02188, 2015.
- 【Automated Feature Engineering】【Expand Reduce】Kanter J M, Veeramachaneni K. [Deep feature synthesis: Towards automating data science endeavors](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf)[C]//Data Science and Advanced Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on. IEEE, 2015: 1-10.

#### 2016
- 【Architecture Search】Mendoza H, Klein A, Feurer M, et al. [Towards automatically-tuned neural networks](http://proceedings.mlr.press/v64/mendoza_towards_2016.html)[C]//Workshop on Automatic Machine Learning. 2016: 58-65.
- 【Hyperparameter Optimization】Fabian Pedregosa. [Hyperparameter optimization with approximate gradient](https://arxiv.org/abs/1602.02355)[J]. arXiv preprint arXiv:1602.02355, 2016.
- 【Hyperparameter Optimization】【Random Search】Li L, Jamieson K, DeSalvo G, et al. [Hyperband: A novel bandit-based approach to hyperparameter optimization](https://arxiv.org/abs/1603.06560)[J]. arXiv preprint arXiv:1603.06560, 2016.
- 【Architecture Search】Loshchilov I, Hutter F. [CMA-ES for hyperparameter optimization of deep neural networks](https://arxiv.org/abs/1604.07269)[J]. arXiv preprint arXiv:1604.07269, 2016.
- 【Hyperparameter Optimization】Julien-Charles Lévesque, Christian Gagné, Robert Sabourin. [Bayesian Hyperparameter Optimization for Ensemble Learning](https://arxiv.org/abs/1605.06394)[J]. arXiv preprint arXiv:1605.06394, 2016.
- 【Make it more efficient】Klein A, Falkner S, Bartels S, et al. [Fast bayesian optimization of machine learning hyperparameters on large datasets](http://proceedings.mlr.press/v54/klein17a.html)[J]. arXiv preprint arXiv:1605.07079, 2016.
- 【Architecture Search】【Meta Learning】Li K, Malik J. [Learning to optimize](https://arxiv.org/pdf/1606.01885.pdf)[J]. arXiv preprint arXiv:1606.01885, 2016.
- 【Architecture Search】Saxena S, Verbeek J. [Convolutional neural fabrics](https://arxiv.org/abs/1606.02492)[C]//Advances in Neural Information Processing Systems. 2016: 4053-4061.
- 【Architecture Search】【Reinforcement Learning】Cortes, Corinna, Gonzalvo, Xavi, Kuznetsov, Vitaly, Mohri, Mehryar, and Yang, Scott. [Adanet: Adaptive structural learning of artificial neural networks](https://arxiv.org/abs/1607.01097). arXiv preprint arXiv:1607.01097, 2016.
- 【Hyperparameter Optimization】Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, Christine Annette Shoemaker. [Efficient Hyperparameter Optimization of Deep Learning Algorithms Using Deterministic RBF Surrogates](https://arxiv.org/abs/1607.08316)[J]. arXiv preprint arXiv:1607.08316, 2016.
- 【Hyperparameter Optimization】【Transfer Learning】Ilija Ilievski, Jiashi Feng. [Hyperparameter Transfer Learning through Surrogate Alignment for Efficient Deep Neural Network Training](https://arxiv.org/abs/1608.00218). arXiv preprint arXiv:1608.00218, 2016.
- 【Architecture Search】【Reinforcement Learning】Zoph B, Le Q V. [Neural architecture search with reinforcement learning](https://arxiv.org/abs/1611.01578)[J]. arXiv preprint arXiv:1611.01578, 2016.
- 【Architecture Search】Baker B, Gupta O, Naik N, et al. [Designing neural network architectures using reinforcement learning](https://arxiv.org/abs/1611.02167)[J]. arXiv preprint arXiv:1611.02167, 2016.
- 【Make it more efficient】Klein A, Falkner S, Springenberg J T, et al. [Learning curve prediction with Bayesian neural networks](http://ml.informatik.uni-freiburg.de/papers/17-ICLR-LCNet.pdf)[J]. 2016.
- 【Hyperparameter Optimization】【Transfer Learning】Wistuba M, Schilling N, Schmidt-Thieme L. [Hyperparameter optimization machines](http://ieeexplore.ieee.org/abstract/document/7796889/)[C]//Data Science and Advanced Analytics (DSAA), 2016 IEEE International Conference on. IEEE, 2016: 41-50.
- 【Hyperparameter Optimization】【Transfer Learning】Joy T T, Rana S, Gupta S K, et al. [Flexible transfer learning framework for bayesian optimisation](https://link.springer.com/chapter/10.1007/978-3-319-31753-3_9)[C]//Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, Cham, 2016: 102-114.
- 【Hyperparameter Optimization】【Bayesian Optimization】Wistuba M, Schilling N, Schmidt-Thieme L. [Two-stage transfer surrogate model for automatic hyperparameter optimization](https://link.springer.com/chapter/10.1007/978-3-319-46128-1_13)[C]//Joint European conference on machine learning and knowledge discovery in databases. Springer, Cham, 2016: 199-214.
- 【Hyperparameter Optimization】【Bayesian Optimization】Mendoza H, Klein A, Feurer M, et al. [Towards automatically-tuned neural networks](http://aad.informatik.uni-freiburg.de/papers/16-AUTOML-AutoNet.pdf)[C]//Workshop on Automatic Machine Learning. 2016: 58-65.
- 【Hyperparameter Optimization】【Bayesian Optimization】Shahriari B, Swersky K, Wang Z, et al. [Taking the human out of the loop: A review of bayesian optimization](http://ieeexplore.ieee.org/document/7352306/)[J]. Proceedings of the IEEE, 2016, 104(1): 148-175.
- 【Hyperparameter Optimization】【Bayesian Optimization】Schilling N, Wistuba M, Schmidt-Thieme L. [Scalable hyperparameter optimization with products of gaussian process experts](https://link.springer.com/chapter/10.1007/978-3-319-46128-1_3)[C]//Joint European conference on machine learning and knowledge discovery in databases. Springer, Cham, 2016: 33-48.
- 【Hyperparameter Optimization】【Bayesian Optimization】Springenberg J T, Klein A, Falkner S, et al. [Bayesian optimization with robust bayesian neural networks](https://papers.nips.cc/paper/6117-bayesian-optimization-with-robust-bayesian-neural-networks.pdf)[C]//Advances in Neural Information Processing Systems. 2016: 4134-4142.
- 【Automated Feature Engineering】【Hierarchical Organization of Transformations】Khurana U, Turaga D, Samulowitz H, et al. [Cognito: Automated feature engineering for supervised learning](http://ieeexplore.ieee.org/document/7836821/)[C]//Data Mining Workshops (ICDMW), 2016 IEEE 16th International Conference on. IEEE, 2016: 1304-1307.
- 【Automated Feature Engineering】【Expand Reduce】Katz G, Shin E C R, Song D. [Explorekit: Automatic feature generation and selection](http://ieeexplore.ieee.org/document/7837936/)[C]//Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 2016: 979-984.
- 【Automated Feature Engineering】【Expand Reduce】Khurana U, Nargesian F, Samulowitz H, et al. [Automating Feature Engineering](http://workshops.inf.ed.ac.uk/nips2016-ai4datasci/papers/NIPS2016-AI4DataSci_paper_13.pdf)[J]. Transformation, 2016, 10(10): 10.

#### 2017
- 【Architecture Search】【Evolutionary Algorithms】【more efficient】Miikkulainen, Risto, Liang, Jason, Meyerson, Elliot,Rawal, Aditya, Fink, Dan, Francon, Olivier, Raju,Bala, Navruzyan, Arshak, Duffy, Nigel, and Hodjat,Babak. [Evolving deep neural networks](https://arxiv.org/abs/1703.00548). arXiv preprint arXiv:1703.00548, 2017
- 【Architecture Search】【Hyperparameter Optimization】【Evolutionary Algorithms】Real E, Moore S, Selle A, et al. [Large-scale evolution of image classifiers](https://arxiv.org/abs/1703.01041)[J]. arXiv preprint arXiv:1703.01041, 2017.
- 【Architecture Search】【Evolutionary Algorithms】Xie, Lingxi and Yuille, Alan. [Genetic cnn](https://arxiv.org/abs/1703.01513). arXiv preprint arXiv:1703.01513, 2017.
- 【Hyperparameter Optimization】Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil. [Forward and Reverse Gradient-Based Hyperparameter Optimization](https://arxiv.org/abs/1703.01785)[J]. arXiv preprint arXiv:1703.01785, 2017.
- 【Hyperparameter Optimization】【Lipschitz Functions】Malherbe C, Vayatis N. [Global optimization of Lipschitz functions](https://arxiv.org/pdf/1703.02628.pdf)[J]. arXiv preprint arXiv:1703.02628, 2017.
- 【Hyperparameter Optimization】【Meta Learning】Ben Goertzel, Nil Geisweiller, Chris Poulin. [Metalearning for Feature Selection
](https://arxiv.org/abs/1703.06990). arXiv preprint arXiv:1703.06990, 2017.
- 【Architecture Search】Suganuma, Masanori, Shirakawa, Shinichi, and Nagao, Tomoharu. [A genetic programming approach to designing convolutional neural network architectures](https://arxiv.org/abs/1704.00764). arXiv preprint arXiv:1704.00764, 2017.
- 【Architecture Search】Negrinho, Renato and Gordon, Geoff. [Deeparchitect: Automatically designing and training deep architectures](https://arxiv.org/abs/1704.08792). arXiv preprint arXiv:1704.08792, 2017.
- 【Hyperparameter Optimization】Gonzalo Diaz, Achille Fokoue, Giacomo Nannicini, Horst Samulowitz. [An effective algorithm for hyperparameter optimization of neural networks](https://arxiv.org/abs/1705.08520)[J]. arXiv preprint arXiv:1705.08520, 2017.
- 【Automated Feature Engineering】【Expand Reduce】Lam H T, Thiebaut J M, Sinn M, et al. [One button machine for automating feature engineering in relational databases](https://arxiv.org/pdf/1706.00327.pdf)[J]. arXiv preprint arXiv:1706.00327, 2017.
- 【Architecture Search】Hazan E, Klivans A, Yuan Y. [Hyperparameter Optimization: A Spectral Approach](https://arxiv.org/abs/1706.00764
)[J]. arXiv preprint arXiv:1706.00764, 2017.
- 【Hyperparameter Optimization】Jesse Dodge, Kevin Jamieson, Noah A. Smith. [Open Loop Hyperparameter Optimization and Determinantal Point Processes	Machine Learning](https://arxiv.org/abs/1706.01566)[J]. arXiv preprint arXiv:1706.01566, 2017.
- 【Architecture Search】Huang, Furong, Ash, Jordan, Langford, John, and Schapire, Robert. [Learning deep resnet blocks sequentially using boosting theory](https://arxiv.org/abs/1706.04964). arXiv preprint arXiv:1706.04964, 2017
- 【Hyperparameter Optimization】【Meta Learning】Fábio Pinto, Vítor Cerqueira, Carlos Soares, João Mendes-Moreira. [autoBagging: Learning to Rank Bagging Workflows with Metalearning](https://arxiv.org/abs/1706.09367)[J].  arXiv preprint arXiv:1706.09367, 2017.
- 【Architecture Search】Cai H, Chen T, Zhang W, et al. [Efficient Architecture Search by Network Transformation](https://arxiv.org/abs/1707.04873)[J].  arXiv preprint arXiv:1707.04873, 2017. 
- 【Architecture Search】【Transfer Learning】Zoph B, Vasudevan V, Shlens J, et al. [Learning transferable architectures for scalable image recognition](https://arxiv.org/abs/1707.07012)[J]. arXiv preprint arXiv:1707.07012, 2017.
- 【Architecture Search】【more efficient】Brock A, Lim T, Ritchie J M, et al. [SMASH: one-shot model architecture search through hypernetworks](https://arxiv.org/abs/1708.05344)[J]. arXiv preprint arXiv:1708.05344, 2017.
- 【Architecture Search】【reinforcement learning】Zhong, Zhao, Yan, Junjie, and Liu, Cheng-Lin. [Practical network blocks design with q-learning](https://arxiv.org/abs/1708.05552). arXiv preprint arXiv:1708.05552, 2017.
- 【Architecture Search】【reinforcement learning】Bello I, Zoph B, Vasudevan V, et al. [Neural optimizer search with reinforcement learning](https://arxiv.org/abs/1709.07417)[J]. arXiv preprint arXiv:1709.07417, 2017.
- 【Automated Feature Engineering】【Reinforcement Learning】Khurana U, Samulowitz H, Turaga D. [Feature Engineering for Predictive Modeling using Reinforcement Learning](https://arxiv.org/pdf/1709.07150.pdf)[J]. arXiv preprint arXiv:1709.07150, 2017.
- 【Hyperparameter Optimization】Jungtaek Kim, Saehoon Kim, Seungjin Choi. [Learning to Warm-Start Bayesian Hyperparameter Optimization](https://arxiv.org/abs/1710.06219). [J]  arXiv preprint arXiv:1709.07150, 2017.
- 【Architecture Search】Liu H, Simonyan K, Vinyals O, et al. [Hierarchical representations for efficient architecture search](https://arxiv.org/abs/1711.00436). [J] arXiv preprint arXiv:1710.06219, 2017.
- 【Architecture Search】【Evolutionary Algorithms】Liu, Hanxiao, Simonyan, Karen, Vinyals, Oriol, Fernando,Chrisantha, and Kavukcuoglu, Koray. [Hierarchical representations for efficient architecture search](https://arxiv.org/abs/1711.00436). arXiv preprint arXiv:1711.00436, 2017b.
- 【Architecture Search】【Local Search】Elsken T, Metzen J H, Hutter F. [Simple and efficient architecture search for convolutional neural networks](https://arxiv.org/abs/1711.04528)[J]. arXiv preprint arXiv:1711.04528, 2017.
- 【Architecture Search】Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, Koray Kavukcuoglu. [Population Based Training of Neural Networks](https://arxiv.org/abs/1711.09846)[J]. arXiv preprint arXiv:1711.09846, 2017.
- 【Architecture Search】【more efficient】Liu C, Zoph B, Shlens J, et al. [Progressive neural architecture search](https://arxiv.org/abs/1712.00559)[J]. arXiv preprint arXiv:1712.00559, 2017.
- 【Architecture Search】Wistuba M. [Finding Competitive Network Architectures Within a Day Using UCT](https://arxiv.org/abs/1712.07420)[J]. arXiv preprint arXiv:1712.07420, 2017.
- 【Hyperparameter Optimization】【Particle Swarm Optimization】Lorenzo P R, Nalepa J, Kawulok M, et al. [Particle swarm optimization for hyper-parameter selection in deep neural networks](https://dl.acm.org/citation.cfm?id=3071208)[C]//Proceedings of the Genetic and Evolutionary Computation Conference. ACM, 2017: 481-488.
- 【Frameworks】Swearingen T, Drevo W, Cyphers B, et al. [ATM: A distributed, collaborative, scalable system for automated machine learning](https://cyphe.rs/static/atm.pdf)[C]//IEEE International Conference on Big Data. 2017.
- 【Frameworks】Golovin D, Solnik B, Moitra S, et al. [Google vizier: A service for black-box optimization](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/46180.pdf)[C]//Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017: 1487-1495.
- 【Automated Feature Engineering】【Meta Learning】Nargesian F, Samulowitz H, Khurana U, et al. [Learning feature engineering for classification](https://www.ijcai.org/proceedings/2017/0352.pdf)[C]//Proceedings of the 26th International Joint Conference on Artificial Intelligence. AAAI Press, 2017: 2529-2535.
- 【Miscellaneous】Martin Wistuba, et al. [Automatic Frankensteining: Creating Complex Ensembles Autonomously](http://epubs.siam.org/doi/pdf/10.1137/1.9781611974973.83)

#### 2018
- 【Architecture Search】【Evolutionary Algorithms】Real E, Aggarwal A, Huang Y, et al. [Regularized Evolution for Image Classifier Architecture Search](https://arxiv.org/abs/1802.01548)[J]. arXiv preprint arXiv:1802.01548, 2018.
- 【Architecture Search】【Reinforcement Learning】Pham H, Guan M Y, Zoph B, et al. [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/abs/1802.03268)[J]. arXiv preprint arXiv:1802.03268, 2018.
- 【Architecture Search】Kandasamy K, Neiswanger W, Schneider J, et al. [Neural Architecture Search with Bayesian Optimisation and Optimal Transport](https://arxiv.org/abs/1802.07191)[J]. arXiv preprint arXiv:1802.07191, 2018.
- 【Hyperparameter Optimization】Lorraine, Jonathan, and David Duvenaud. [Stochastic Hyperparameter Optimization through Hypernetworks](https://arxiv.org/abs/1802.09419) arXiv preprint arXiv:1802.09419 (2018).
- 【Hyperparameter Optimization】【Evolutionary Algorithms】Chen B, Wu H, Mo W, et al. [Autopythonstacker: A Compositional Evolutionary Learning System](https://arxiv.org/pdf/1803.00684.pdf)[J]. arXiv preprint arXiv:1803.00684, 2018.
- 【more efficient】Wong C, Houlsby N, Lu Y, et al. [Transfer Automatic Machine Learning](https://arxiv.org/abs/1803.02780)[J]. arXiv preprint arXiv:1803.02780, 2018.
- 【Architecture Search】Kamath P, Singh A, Dutta D. [Neural Architecture Construction using EnvelopeNets](https://arxiv.org/abs/1803.06744)[J]. arXiv preprint arXiv:1803.06744, 2018.
- 【Hyperparameter Optimization】Cui, Henggang, Gregory R. Ganger, and Phillip B. Gibbons. [MLtuner: System Support for Automatic Machine Learning Tuning](https://arxiv.org/abs/1803.07445) arXiv preprint arXiv:1803.07445 (2018).
- 【more efficient】Bennani-Smires K, Musat C, Hossmann A, et al. [GitGraph-from Computational Subgraphs to Smaller Architecture Search Spaces](https://openreview.net/pdf?id=rkiO1_1Pz)[J]. 2018.
- 【Multi-Objective NAS】Dong J D, Cheng A C, Juan D C, et al. [PPP-Net: Platform-aware Progressive Search for Pareto-optimal Neural Architectures](https://openreview.net/pdf?id=B1NT3TAIM)[J]. 2018.
- 【more efficient】Bowen Baker, et al. Baker B, Gupta O, Raskar R, et al. [Accelerating Neural Architecture Search using Performance Prediction](https://openreview.net/pdf?id=BJypUGZ0Z)[J]. 2018.
- 【Architecture Search】Huang, Siyu, et al. [GNAS: A Greedy Neural Architecture Search Method for Multi-Attribute Learning.](https://arxiv.org/abs/1804.06964) arXiv preprint arXiv:1804.06964 (2018).


---
## SURVEY
- Elsken T, Metzen J H, Hutter F. [Neural architecture search: A survey](http://www.jmlr.org/papers/volume20/18-598/18-598.pdf)[J]. arXiv preprint arXiv:1808.05377, 2018.
- Yao Q, Wang M, Chen Y, et al. [Taking human out of learning applications: A survey on automated machine learning](https://arxiv.org/pdf/1810.13306)[J]. arXiv preprint arXiv:1810.13306, 2018.
- Zöller M A, Huber M F. [Benchmark and Survey of Automated Machine Learning Frameworks](https://arxiv.org/pdf/1904.12054)[J]. arXiv preprint arXiv:1904.12054, 2019.
- Wistuba M, Rawat A, Pedapati T. [A survey on neural architecture search](https://arxiv.org/pdf/1905.01392)[J]. arXiv preprint arXiv:1905.01392, 2019.
- Elshawi R, Maher M, Sakr S. [Automated machine learning: State-of-the-art and open challenges](https://arxiv.org/pdf/1906.02287)[J]. arXiv preprint arXiv:1906.02287, 2019.
- Chen Y W, Song Q, Hu X. [Techniques for Automated Machine Learning](https://arxiv.org/pdf/1907.08908)[J]. arXiv preprint arXiv:1907.08908, 2019.
- He X, Zhao K, Chu X. [AutoML: A Survey of the State-of-the-Art](https://arxiv.org/pdf/1908.00709.pdf)[J]. arXiv preprint arXiv:1908.00709, 2019.
- Truong A, Walters A, Goodsitt J, et al. [Towards automated machine learning: Evaluation and comparison of automl approaches and tools](https://arxiv.org/pdf/1908.05557.pdf)[J]. arXiv preprint arXiv:1908.05557, 2019.
- Ono J P, Castelo S, Lopez R, et al. [PipelineProfiler: A Visual Analytics Tool for the Exploration of AutoML Pipelines](https://arxiv.org/pdf/2005.00160)[J]. arXiv preprint arXiv:2005.00160, 2020.
- Halvari T, Nurminen J K, Mikkonen T. [Testing the Robustness of AutoML Systems](https://arxiv.org/pdf/2005.02649)[J]. arXiv preprint arXiv:2005.02649, 2020.
- Rippel O, Weninger L, Merhof D. [AutoML Segmentation for 3D Medical Image Data: Contribution to the MSD Challenge 2018](https://arxiv.org/pdf/2005.09978)[J]. arXiv preprint arXiv:2005.09978, 2020.
- Ren P, Xiao Y, Chang X, et al. [A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions](https://arxiv.org/pdf/2006.02903)[J]. arXiv preprint arXiv:2006.02903, 2020.
- Liu Z, Bousquet O, Elisseeff A, et al. [AutoDL Challenge Design and Beta Tests-Towards automatic deep learning](https://hal.archives-ouvertes.fr/hal-01906226/file/AutoDL_challenge_design_and_beta_tests_____towards_Automatic_Deep_Learning.pdf)[C]. 2018.


---
## blogs & articles & book

#### 2008
- 【book】【Meta Learning】Brazdil P, Carrier C G, Soares C, et al. [Metalearning: Applications to data mining](http://www.springer.com/la/book/9783540732624)[M]. Springer Science & Business Media, 2008.

#### 2016
- 【Articles】【Bayesian Optimization】[Bayesian Optimization for Hyperparameter Tuning](https://arimo.com/data-science/2016/bayesian-optimization-hyperparameter-tuning/)

#### 2017
- 【Articles】【Meta Learning】[Learning to learn](http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/)
- 【Articles】【Meta Learning】[Why Meta-learning is Crucial for Further Advances of Artificial Intelligence?](https://chatbotslife.com/why-meta-learning-is-crucial-for-further-advances-of-artificial-intelligence-c2df55959adf)
- 【articles】[automl_aws_data_science](https://alexisperrier.com/aws/2017/12/04/automl_aws_data_science.html)
- 【news】[what-is-automl-promises-vs-realityauto](https://www.iotforall.com/what-is-automl-promises-vs-realityauto/)

#### 2018
- 【book】Sibanjan Das, Umit Mert Cakmak - [Hands-On Automated Machine Learning](https://libgen.io/search.php?req=+automated+machine+learning+&open=0&res=25&view=simple&phrase=1&column=def) (2018, Packt Publishing)

---
## Libraries
[S:Structured Data; I:Image; A:Audio; N:NLP]

- **[mlpapers/automl](https://github.com/mlpapers/automl)**:
- **[shukwong/awesome_automl_libraries](https://github.com/shukwong/awesome_automl_libraries)**:
- **[FeatureLabs/Featuretools](https://github.com/FeatureLabs/featuretools)**:    a good library for automatically engineering features from relational and transactional data
- **[automl/auto-sklearn](https://github.com/automl/auto-sklearn)**:    it's really a drop-in replacement for scikit-learn estimators.
- **[reiinakano/Xcessive](https://github.com/reiinakano/xcessiv)**:    A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling in Python
- **[ray-project/ray](https://github.com/ray-project/ray)**:    A fast and simple framework for building and running distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library
- **[I][keras-team/AutoKeras](https://github.com/keras-team/autokeras)**:    An AutoML system based on Keras. It is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible for everyone
- [I][microsoft/nni](https://github.com/microsoft/nni):    An open source AutoML toolkit for automate machine learning lifecycle, including feature engineering, neural architecture search, model compression and hyper-parameter tuning
- [tensorflow/adanet](https://github.com/tensorflow/adanet):    AdaNet is a lightweight TensorFlow-based framework for automatically learning high-quality models with minimal expert intervention. AdaNet builds on recent AutoML efforts to be fast and flexible while providing learning guarantees. Importantly, AdaNet provides a general framework for not only learning a neural network architecture, but also for learning to ensemble to obtain even better models
- [mindsdb/mindsdb](https://github.com/mindsdb/mindsdb):    MindsDB is an Explainable AutoML framework for developers built on top of Pytorch. It enables you to build, train and test state of the art ML models in as simple as one line of code
- [awslabs/autogluon](https://github.com/awslabs/autogluon):    AutoGluon automates machine learning tasks enabling you to easily achieve strong predictive performance in your applications. With just a few lines of code, you can train and deploy high-accuracy deep learning models on tabular, image, and text data
- [minimaxir/automl-gs](https://github.com/minimaxir/automl-gs):    automl-gs is an AutoML tool which, unlike Microsoft's NNI, Uber's Ludwig, and TPOT, offers a zero code/model definition interface to getting an optimized model and data transformation pipeline in multiple popular ML/DL frameworks, with minimal Python dependencies (pandas + scikit-learn + your framework of choice). automl-gs is designed for citizen data scientists and engineers without a deep statistical background under the philosophy that you don't need to know any modern data preprocessing and machine learning engineering techniques to create a powerful prediction workflow
- [tobegit3hub/advisor](https://github.com/tobegit3hub/advisor):    Advisor is the hyper parameters tuning system for black box optimization
- [ScottfreeLLC/AlphaPy](https://github.com/ScottfreeLLC/AlphaPy):    AlphaPy is a machine learning framework for both speculators and data scientists. It is written in Python with the scikit-learn, pandas, and Keras libraries, as well as many other helpful libraries for feature engineering and visualization
- [Neuraxio/Neuraxle](https://github.com/Neuraxio/Neuraxle):    A Sklearn-like Framework for Hyperparameter Tuning and AutoML in Deep Learning projects. Finally have the right abstractions and design patterns to properly do AutoML. Let your pipeline steps have hyperparameter spaces. Enable checkpoints to cut duplicate calculations. Go from research to production environment easily.
- [IBM/AutoMLPipline.jl](https://github.com/IBM/AutoMLPipeline.jl):    is a package that makes it trivial to create complex ML pipeline structures using simple expressions. It leverages on the built-in macro programming features of Julia to symbolically process, manipulate pipeline expressions, and makes it easy to discover optimal structures for machine learning prediction and classification.
- [HDI-Project/BTB](https://github.com/HDI-Project/BTB):    BTB ("Bayesian Tuning and Bandits") is a simple, extensible backend for developing auto-tuning systems such as AutoML systems. It provides an easy-to-use interface for tuning and selection
- [NVIDIA/Milano](https://github.com/NVIDIA/Milano):    Milano (Machine learning autotuner and network optimizer) is a tool for enabling machine learning researchers and practitioners to perform massive hyperparameters and architecture searches
- [deephyper/deephyper](https://github.com/deephyper/deephyper):    DeepHyper is an automated machine learning (AutoML) package for deep neural networks. It comprises two components: 1) Neural architecture search is an approach for automatically searching for high-performing the deep neural network search_space. 2) Hyperparameter search is an approach for automatically searching for high-performing hyperparameters for a given deep neural network. DeepHyper provides an infrastructure that targets experimental research in neural architecture and hyperparameter search methods, scalability, and portability across HPC systems. It comprises three modules: benchmarks, a collection of extensible and diverse benchmark problems; search, a set of search algorithms for neural architecture search and hyperparameter search; and evaluators, a common interface for evaluating hyperparameter configurations on HPC platforms
- [dataloop-ai/zazuml](https://github.com/dataloop-ai/ZazuML):    This is an easy open-source AutoML framework for object detection. Currently this project contains a model & hyper-parameter tuner, auto augmentations, trial manager and prediction trigger, already loaded with your top preforming model-checkpoint. A working pipeline ready to be plugged into your product, simple as that
- [Ashton-Sidhu/aethos](https://github.com/Ashton-Sidhu/aethos):    Aethos is a library/platform that automates your data science and analytical tasks at any stage in the pipeline. Aethos is, at its core, a uniform API that helps automate analytical techniques from various libaries such as pandas, sci-kit learn, gensim, etc
- [databricks/automl-toolkit](https://github.com/databrickslabs/automl-toolkit):    This package provides a number of different levels of API interaction, from the highest-level "default only" FamilyRunner to low-level APIs that allow for highly customizable workflows to be created for automated ML tuning and Inference
- [flytxtds/AutoGBT](https://github.com/flytxtds/AutoGBT):    AutoGBT stands for Automatically Optimized Gradient Boosting Trees, and is used for AutoML in a lifelong machine learning setting to classify large volume high cardinality data streams under concept-drift. AutoGBT was developed by a joint team ('autodidact.ai') from Flytxt, Indian Institute of Technology Delhi and CSIR-CEERI as a part of NIPS 2018 AutoML Challenge (The 3rd AutoML Challenge: AutoML for Lifelong Machine Learning).
- [HDI-Project/AutobBzaar](https://github.com/HDI-Project/AutoBazaar):    AutoBazaar is an AutoML system created using The Machine Learning Bazaar, a research project and framework for building ML and AutoML systems by the Data To AI Lab at MIT.
- [EpistasisLab/TPOT](https://github.com/EpistasisLab/tpot):    is using genetic programming to find the best performing ML pipelines, and it is built on top of scikit-learn
- [automl/Auto-WEKA](https://github.com/automl/autoweka):    Repository for Auto-WEKA, wich provides automatic selection of models and hyperparameters for WEKA
- [hyperopt/Hyperopt](https://github.com/hyperopt/hyperopt):    Distributed Asynchronous Hyperparameter Optimization in Python, for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.
- [hyperopt/Hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn):    Hyperopt-sklearn is Hyperopt-based model selection among machine learning algorithms in scikit-learn.
- [SigOpt](https://sigopt.com/):    SigOpt is a standardized, scalable, enterprise-grade optimization platform and API designed to unlock the potential of your modeling pipelines. This fully agnostic software solution accelerates, amplifies, and scales the model development process.
- [automl/SMAC3](https://github.com/automl/SMAC3):    SMAC is a tool for algorithm configuration to optimize the parameters of arbitrary algorithms across a set of instances. This also includes hyperparameter optimization of ML algorithms. The main core consists of Bayesian Optimization in combination with a aggressive racing mechanism to efficiently decide which of two configuration performs better
- [HIPS/Spearmint](https://github.com/HIPS/Spearmint):    Spearmint is a software package to perform Bayesian optimization. The Software is designed to automatically run experiments (thus the code name spearmint) in a manner that iteratively adjusts a number of parameters so as to minimize some objective in as few runs as possible
- [Yelp/MOE](https://github.com/Yelp/MOE):    MOE (Metric Optimization Engine) is an efficient way to optimize a system's parameters, when evaluating parameters is time-consuming or expensive
- [automl/RoBO](https://github.com/automl/RoBO):    RoBO uses the Gaussian processes library george and the random forests library pyrfr.
- [fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization):    This is a constrained global optimization package built upon bayesian inference and gaussian process, that attempts to find the maximum value of an unknown function in as few iterations as possible. This technique is particularly suited for optimization of high cost functions, situations where the balance between exploration and exploitation is important
- [scikit-optimize/Scikit-Optimize](https://github.com/scikit-optimize/scikit-optimize):    Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts
- [zygmuntz/HyperBand](https://github.com/zygmuntz/hyperband):    The goal is to provide a fully functional implementation of Hyperband, as well as a number of ready to use functions for a number of models (classifiers and regressors)
- [rmcantin/BayesOpt](https://github.com/rmcantin/bayesopt):    BayesOpt is an efficient implementation of the Bayesian optimization methodology for nonlinear optimization, experimental design and hyperparameter tunning
- [claesenm/Optunity](https://github.com/claesenm/optunity):    Optunity is a library containing various optimizers for hyperparameter tuning. Hyperparameter tuning is a recurrent problem in many machine learning tasks, both supervised and unsupervised. Tuning examples include optimizing regularization or kernel parameters
- 【Commercial】[Cloud AutoML](https://cloud.google.com/automl/):
- [H2O-offical website](https://www.h2o.ai/);  [H2O-github](https://github.com/h2oai):    Open Source Fast Scalable Machine Learning Platform For Smarter Applications: Deep Learning, Gradient Boosting & XGBoost, Random Forest, Generalized Linear Modeling (Logistic Regression, Elastic Net), K-Means, PCA, Stacked Ensembles, Automatic Machine Learning (AutoML), etc
- 【Commercial】[DataRobot](https://www.datarobot.com/):    Learn from an all-star lineup of expert speakers how to best leverage AI today to build business resilience, reduce costs, and speed time to results
- [MLJAR](https://mljar.com/):    An Automated Machine Learning (AutoML) python package for tabular data. It can handle: Binary Classification, MultiClass Classification and Regression. It provides explanations and markdown reports.
- [MateLabs](http://matelabs.in/):
- [Angle-ml/automl](https://github.com/Angel-ML/automl):
- [accurat/ackeras](https://github.com/accurat/ackeras):
- [naszilla/bananas](https://github.com/naszilla/bananas):
- [auto-flow/auto-flow](https://github.com/auto-flow/auto-flow):
- [r-tensorflow/autokeras](https://github.com/r-tensorflow/autokeras):
- [IBM/lale](https://github.com/IBM/lale):
- [awslabs/adatune](https://github.com/awslabs/adatune):
- [CiscoAI/amla](https://github.com/CiscoAI/amla):
- [xiaomi-automl/fairdarts](https://github.com/xiaomi-automl/FairDARTS):
- [cod3licious/autofeat](https://github.com/cod3licious/autofeat):
- [google-research/morphnet](https://github.com/google-research/morph-net):
- [ypeleg/HungaBunga](https://github.com/ypeleg/HungaBunga):
- [Alex-Lekov/AutoML_Alex](https://github.com/Alex-Lekov/AutoML_Alex):
- [logicalclocks/maggy](https://github.com/logicalclocks/maggy):
- [crawles/automl_service](https://github.com/crawles/automl_service):
- [kakaobrain/fast-autoaugment](https://github.com/kakaobrain/fast-autoaugment):
- [quark0/DARTS](https://github.com/quark0/darts):
- [joeddav/DEvol](https://github.com/joeddav/devol):    DEvol (DeepEvolution) is a basic proof of concept for genetic architecture search in Keras. The current setup is designed for classification problems, though this could be extended to include any other output type as well.:DEvol (DeepEvolution) is a basic proof of concept for genetic architecture search in Keras. The current setup is designed for classification problems, though this could be extended to include any other output type as well.
- [laic-ufmg/recipe](https://github.com/laic-ufmg/Recipe):
- [ccnt-glaucus/glaucus](https://github.com/ccnt-glaucus/glaucus/blob/master/README_CN.md):
- [mb706/automlr](https://github.com/mb706/automlr):
- [AutoViML/auto_ts](https://github.com/AutoViML/Auto_TS):
- [gfluz94/aautoml-gfluz](https://github.com/gfluz94/automl-gfluz):
- [giantcroc/featuretoolsOnSpark](https://github.com/giantcroc/featuretoolsOnSpark):
- [pierre-chaville/automlk](https://github.com/pierre-chaville/automlk):
- [georgianpartners/foreshadow](https://github.com/georgianpartners/foreshadow):
- [societe-generale/aikit](https://github.com/societe-generale/aikit):
- [onepanelio/automl](https://github.com/onepanelio/automl):
- [SoftwareAG/mlw](https://github.com/SoftwareAG/MLW):
- [souryadey/deep-n-cheap](https://github.com/souryadey/deep-n-cheap):
- [ksachdeva/scikit-nni](https://github.com/ksachdeva/scikit-nni):
- [deil87/automl-genetic](https://github.com/deil87/automl-genetic):
- [CleverInsight/cognito](https://github.com/CleverInsight/cognito):
- [kxsystems/automl](https://github.com/KxSystems/automl):
- [bhat-prashant/reinforceML](https://github.com/bhat-prashant/reinforceML):
- [thomas-young-2013/alpha-ml](https://github.com/thomas-young-2013/alpha-ml):
- [wywongbd/autocluster](https://github.com/wywongbd/autocluster):
- [Media-Smart/volkstuner](https://github.com/Media-Smart/volkstuner):
- [mihaianton/automl](https://github.com/MihaiAnton/AutoML):
- [gomerudo/automl](https://github.com/gomerudo/auto-ml):
- [SaltWaterStudio/modgen](https://github.com/SaltWaterStudio/modgen):
- [epeters3/skplumber](https://github.com/epeters3/skplumber):
- [takezoe/predictionio-template-automl](https://github.com/takezoe/predictionio-template-automl):
- [LGE-ARC-AdvancedAI/Auptimizer](https://github.com/LGE-ARC-AdvancedAI/auptimizer):
- [keras-team/keras-tuner](https://github.com/keras-team/keras-tuner):    Hyperparameter tuning for humans
- [tristandeleu/pytorch-meta](https://github.com/tristandeleu/pytorch-meta):    A collection of extensions and data-loaders for few-shot learning & meta-learning in PyTorch. Torchmeta contains popular meta-learning benchmarks, fully compatible with both torchvision and PyTorch's DataLoader
- [learnables/learn2learn](https://github.com/learnables/learn2learn):    PyTorch Meta-learning Library for Researchers
- [dragonfly/Dragonfly](https://github.com/dragonfly/dragonfly):    An open source python library for scalable Bayesian optimisation.
- [automl/Auto-Pytorch](https://github.com/automl/Auto-PyTorch):    Automatic architecture search and hyperparameter optimization for PyTorch
- [kubeflow/katib](https://github.com/kubeflow/katib):    Katib is a Kubernetes-based system for Hyperparameter Tuning and Neural Architecture Search. Katib supports a number of ML frameworks, including TensorFlow, Apache MXNet, PyTorch, XGBoost, and others
- [uber/ludwig](https://github.com/uber/ludwig):    Ludwig is a toolbox built on top of TensorFlow that allows users to train and test deep learning models without the need to write code
- [BartekPog/Automl-library](https://github.com/BartekPog/AutoML-Library):
- [tqichun/distributed-SMAC3](https://github.com/tqichun/distributed-SMAC3):
- [linxihui/lazyML](https://github.com/linxihui/lazyML):
- [MainRo/xgbtune](https://github.com/MainRo/xgbtune):
- [gdikov/hypertunity](https://github.com/gdikov/hypertunity):    A toolset for black-box hyperparameter optimisation.
- [automl/HPOlib2](https://github.com/automl/HPOlib2):    HPOlib2 is a library for hyperparameter optimization and black box optimization benchmarks.
- [DataSystemsGroupUT/SmartML](https://github.com/DataSystemsGroupUT/SmartML):
- [ziyuw/rembo](https://github.com/ziyuw/rembo):    Bayesian optimization in high-dimensions via random embedding.
- [optuna/optuna](https://github.com/optuna/optuna):    Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Thanks to our define-by-run API, the code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters
- [automl/RoBO](https://github.com/automl/RoBO):    RoBO a Robust Bayesian Optimization framework
- [AutoCross](https://www.4paradigm.com/):
- [HyperTune](https://bit.ly/2IMsECx):
- [DarwinML](http://iqubic.net/)
- [paper][AutoStacker](https://arxiv.org/abs/1803.00684):
- [paper][AlphaD3M](https://www.cs.columbia.edu/~idrori/AlphaD3M.pdf):
- [paper][VDS](https://confer.csail.mit.edu/sigmod2019/papers):
- [paper][ExploreKit](https://people.eecs.berkeley.edu/~dawnsong/papers/icdm-2016.pdf):

### Distributed Frameworks
- [MLBase](http://www.mlbase.org/):
- [HDI-Project/ATM](https://github.com/HDI-Project/ATM):
- [AxeldeRomblay/MLBox](https://github.com/AxeldeRomblay/MLBox): is another AutoML library and it supports distributed data processing, cleaning, formatting, and state-of-the-art algorithms such as LightGBM and XGBoost. It also supports model stacking, which allows you to combine an information ensemble of models to generate a new model aiming to have better performance than the individual models.
- [nginyc/rafiki](https://github.com/nginyc/rafiki):
- [salesforce/TransmogrifAI](https://github.com/salesforce/TransmogrifAI):
- [HDI-Project/ATMSeer](https://github.com/HDI-Project/ATMSeer):
- [DataSystemsGroupUT/D-SmartML](https://github.com/DataSystemsGroupUT/Distributed-SmartML):
- [Databricks](https://databricks.com/product/automl-on-databricks#resource-link):
- [automl/HpBandSter](https://github.com/automl/HpBandSter):a distributed Hyperband implementation on Steroids



## Projects
- [DeepWisdom/AutoDL-cn](https://github.com/DeepWisdom/AutoDL)
- [mit-han-lab/once-for-all](https://github.com/mit-han-lab/once-for-all)
- [NoamRosenberg/autodeeplab](https://github.com/NoamRosenberg/autodeeplab)
- [microsoft/forecasting](https://github.com/microsoft/forecasting)
- [lightforever/mlcomp](https://github.com/lightforever/mlcomp)
- [e2its/gdayf-core](https://github.com/e2its/gdayf-core)
- [AutoViML/AutoViz](https://github.com/AutoViML/AutoViz)
- [paypal/autskearn-zeroconf](https://github.com/paypal/autosklearn-zeroconf)
- [kakaobrain/autoclint](https://github.com/kakaobrain/autoclint)
- [cmusatyalab/opentpod](https://github.com/cmusatyalab/OpenTPOD)
- [pfnet-research/autpgbt-alt](https://github.com/pfnet-research/autogbt-alt)
- [arberzela/efficientnas](https://github.com/arberzela/EfficientNAS)
- [ealcobaca/pymfe](https://github.com/ealcobaca/pymfe)
- [TAMU-VITA/autospeech](https://github.com/TAMU-VITA/AutoSpeech)
- [java][fmohr/AILibs](https://github.com/fmohr/AILibs)
- [DAI-Lab/cardea](https://github.com/DAI-Lab/Cardea)
- [datamllab/autokaggle](https://github.com/datamllab/autokaggle)
- [signals-dev/greenguard](https://github.com/signals-dev/GreenGuard)
- [MaximilianJohannesObpacher/automl_server](https://github.com/MaximilianJohannesObpacher/automl_server)
- [thomas-young-2013/soln-ml](https://github.com/thomas-young-2013/soln-ml)
- [BeelGroup/auto-surprise](https://github.com/BeelGroup/Auto-Surprise)
- [melodyguan/ENAS](https://github.com/melodyguan/enas)
- [renqianluo/NAO](https://github.com/renqianluo/NAO)
- [laic-ufmg/automlc](https://github.com/laic-ufmg/automlc)
- [AlexImb/automl-streams](https://github.com/AlexImb/automl-streams)
- [knowledge-learning/hp-optimization](https://github.com/knowledge-learning/hp-optimization)
- [magnusax/magnusax/automl](https://github.com/magnusax/AutoML)
- [dstreamsai/ALEX_AutoML](https://github.com/dstreamsai/AutoML)
- [nitishkthakur/nitishkthakur/automlib](https://github.com/nitishkthakur/automlib)
- [DataSystemsGroupUT/iSmartML](https://github.com/DataSystemsGroupUT/ismartml)
- [udellgroup/Oboe](https://github.com/udellgroup/oboe)
- [automl/Auto-Pytorch](https://github.com/automl/Auto-PyTorch)
- [plabig/Dino](https://github.com/plabig/Dino)

## benchmark
- [Alex-Lekov/automl-benchmark](https://github.com/Alex-Lekov/AutoML-Benchmark)
- [jonathankrauss/Automl-benchmark](https://jonathankrauss.github.io/AutoML-Benchmark/)
- [openml/automlbenckMark](https://github.com/openml/automlbenchmark)
- [jessecui/automl-benckmarking](https://github.com/jessecui/automl-benchmarking)
- [google-research/nasbench](https://github.com/google-research/nasbench)
- [automl/nas_benchmarks](https://github.com/automl/nas_benchmarks)
- [gaocegege.com/Blog/katib-new](http://gaocegege.com/Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/katib-new#%E6%80%BB%E7%BB%93%E4%B8%8E%E5%88%86%E6%9E%90)
