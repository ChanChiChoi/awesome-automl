# awesome-automl   
collecting related resources of automated machine learning here. some links were from below:
- [hibayesian/awesome-automl-papers](https://github.com/hibayesian/awesome-automl-papers)   
- [literature-on-neural-architecture-search](http://www.ml4aad.org/literature-on-neural-architecture-search/)
- [Algorithm Configuration Literature](http://aclib.net/acbib/)

this papers or books or slides are ordered by years, before each entity is the theme the entity belonged, if you want to choice one theme, e.g. "Architecture Search", you can ctrl+F then highlight the papers.   
themes are as follow:   
【Architecture Search】;【Random Search】; 【Evolutionary Algorithms】;     
【Hyperparameter Optimization】;【Bayesian Optimization】  
【Multi-Objective NAS】;  
【Transfer Learning】;  
【Meta Learning】;  
【Lipschitz Functions】;  
【Particle Swarm Optimization】;  
【Local Search】;  
【Miscellaneous】  

---
#### 2008
- 【book】【Meta Learning】Brazdil P, Carrier C G, Soares C, et al. [Metalearning: Applications to data mining](http://www.springer.com/la/book/9783540732624)[M]. Springer Science & Business Media, 2008.
- 【Tutorials】【Meta Learning】[Metalearning - A Tutorial](https://pdfs.semanticscholar.org/5794/1a4891f673cadf06fba02419372aad85c3bb.pdf)
- 【Particle Swarm Optimization】Lin S W, Ying K C, Chen S C, et al. [Particle swarm optimization for parameter determination and feature selection of support vector machines](http://www.sciencedirect.com/science/article/pii/S0957417407003752)[J]. Expert systems with applications, 2008, 35(4): 1817-1824.
- 【Meta Learning】Smith-Miles K A. [Cross-disciplinary perspectives on meta-learning for algorithm selection](https://dl.acm.org/citation.cfm?id=1456656)[J]. ACM Computing Surveys (CSUR), 2009, 41(1): 6.

#### 2009 
- 【Local Search】Hutter F, Hoos H H, Leyton-Brown K, et al. [ParamILS: an automatic algorithm configuration framework](https://arxiv.org/pdf/1401.3492.pdf)[J]. Journal of Artificial Intelligence Research, 2009, 36: 267-306.

#### 2010
- 【Bayesian Optimization】Brochu E, Cora V M, De Freitas N. [A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning](https://arxiv.org/pdf/1012.2599v1.pdf)[J]. arXiv preprint arXiv:1012.2599, 2010.

#### 2011
- 【Random Search】Bergstra J S, Bardenet R, Bengio Y, et al. [Algorithms for hyper-parameter optimization](https://dl.acm.org/citation.cfm?id=2986743)[C]//Advances in neural information processing systems. 2011: 2546-2554.
- 【Hyperparameter Optimization】【Bayesian Optimization】Hutter F, Hoos H H, Leyton-Brown K. [Sequential model-based optimization for general algorithm configuration](https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf)[C]//International Conference on Learning and Intelligent Optimization. Springer, Berlin, Heidelberg, 2011: 507-523.

#### 2012
- 【Architecture Search】Snoek J, Larochelle H, Adams R P. [Practical bayesian optimization of machine learning algorithms](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf  
)[C]//Advances in neural information processing systems. 2012: 2951-2959.
- 【Random Search】Bergstra J, Bengio Y. [Random search for hyper-parameter optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305.
- 【Hyperparameter Optimization】【Bayesian Optimization】Snoek J, Larochelle H, Adams R P. [Practical bayesian optimization of machine learning algorithms](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)[C]//Advances in neural information processing systems. 2012: 2951-2959.

#### 2013
- 【Transfer Learning】Bardenet R, Brendel M, Kégl B, et al. [Collaborative hyperparameter tuning](http://proceedings.mlr.press/v28/bardenet13.pdf)[C]//International Conference on Machine Learning. 2013: 199-207.
- 【Hyperparameter Optimization】【Bayesian Optimization】Bergstra J, Yamins D, Cox D D. [Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures](http://proceedings.mlr.press/v28/bergstra13.pdf)[J]. 2013.
- 【Hyperparameter Optimization】【Bayesian Optimization】Thornton C, Hutter F, Hoos H H, et al. [Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms](http://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/autoweka.pdf)[C]//Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2013: 847-855.

#### 2014
- 【Transfer Learning】Yogatama D, Mann G. [Efficient transfer learning method for automatic hyperparameter tuning](https://pdfs.semanticscholar.org/75f2/6734972ebaffc6b43d45abd3048ef75f15a5.pdf)[C]//Artificial Intelligence and Statistics. 2014: 1077-1085.

#### 2015
- 【Architecture Search】Young S R, Rose D C, Karnowski T P, et al. [Optimizing deep learning hyper-parameters through an evolutionary algorithm](https://dl.acm.org/citation.cfm?id=2834896)[C]//Proceedings of the Workshop on Machine Learning in High-Performance Computing Environments. ACM, 2015: 4.
- 【Hyperparameter Optimization】【Bayesian Optimization】Wistuba M, Schilling N, Schmidt-Thieme L. [Sequential model-free hyperparameter tuning](http://ieeexplore.ieee.org/abstract/document/7373431/)[C]//Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015: 1033-1038.
- 【Hyperparameter Optimization】【Bayesian Optimization】Snoek J, Rippel O, Swersky K, et al. [Scalable bayesian optimization using deep neural networks](https://dl.acm.org/citation.cfm?id=3045349)[C]//International conference on machine learning. 2015: 2171-2180.
- 【Hyperparameter Optimization】【Bayesian Optimization】Wistuba M, Schilling N, Schmidt-Thieme L. [Learning hyperparameter optimization initializations](http://ieeexplore.ieee.org/abstract/document/7344817/)[C]//Data Science and Advanced Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on. IEEE, 2015: 1-10.
- 【Hyperparameter Optimization】【Bayesian Optimization】Schilling N, Wistuba M, Drumond L, et al. [Joint model choice and hyperparameter optimization with factorized multilayer perceptrons](http://ieeexplore.ieee.org/abstract/document/7372120/)[C]//Tools with Artificial Intelligence (ICTAI), 2015 IEEE 27th International Conference on. IEEE, 2015: 72-79.
- 【Hyperparameter Optimization】【Bayesian Optimization】Wistuba M, Schilling N, Schmidt-Thieme L. [Hyperparameter search space pruning–a new component for sequential model-based hyperparameter optimization](https://dl.acm.org/citation.cfm?id=2991491)[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2015: 104-119.
- 【Hyperparameter Optimization】【Bayesian Optimization】Schilling N, Wistuba M, Drumond L, et al. [Hyperparameter optimization with factorized multilayer perceptrons](https://link.springer.com/chapter/10.1007/978-3-319-23525-7_6)[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2015: 87-103.
- 【Hyperparameter Optimization】【Bayesian Optimization】Feurer M, Klein A, Eggensperger K, et al. [Efficient and robust automated machine learning](https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf)[C]//Advances in Neural Information Processing Systems. 2015: 2962-2970.

#### 2016
- 【Architecture Search】Mendoza H, Klein A, Feurer M, et al. [Towards automatically-tuned neural networks](http://proceedings.mlr.press/v64/mendoza_towards_2016.html)[C]//Workshop on Automatic Machine Learning. 2016: 58-65.
- 【Architecture Search】【Random Search】Li L, Jamieson K, DeSalvo G, et al. [Hyperband: A novel bandit-based approach to hyperparameter optimization](https://arxiv.org/abs/1603.06560)[J]. arXiv preprint arXiv:1603.06560, 2016.
- 【Architecture Search】Loshchilov I, Hutter F. [CMA-ES for hyperparameter optimization of deep neural networks](https://arxiv.org/abs/1604.07269)[J]. arXiv preprint arXiv:1604.07269, 2016.
- 【Make it more efficient】Klein A, Falkner S, Bartels S, et al. [Fast bayesian optimization of machine learning hyperparameters on large datasets](http://proceedings.mlr.press/v54/klein17a.html)[J]. arXiv preprint arXiv:1605.07079, 2016.
- 【Architecture Search】Saxena S, Verbeek J. [Convolutional neural fabrics](https://arxiv.org/abs/1606.02492)[C]//Advances in Neural Information Processing Systems. 2016: 4053-4061.
- 【Architecture Search】Zoph B, Le Q V. [Neural architecture search with reinforcement learning](https://arxiv.org/abs/1611.01578)[J]. arXiv preprint arXiv:1611.01578, 2016.
- 【Architecture Search】Baker B, Gupta O, Naik N, et al. [Designing neural network architectures using reinforcement learning](https://arxiv.org/abs/1611.02167)[J]. arXiv preprint arXiv:1611.02167, 2016.
- 【Make it more efficient】Klein A, Falkner S, Springenberg J T, et al. [Learning curve prediction with Bayesian neural networks](http://ml.informatik.uni-freiburg.de/papers/17-ICLR-LCNet.pdf)[J]. 2016.
- 【Articles】【Bayesian Optimization】[Bayesian Optimization for Hyperparameter Tuning](https://arimo.com/data-science/2016/bayesian-optimization-hyperparameter-tuning/)
- 【Transfer Learning】Wistuba M, Schilling N, Schmidt-Thieme L. [Hyperparameter optimization machines](http://ieeexplore.ieee.org/abstract/document/7796889/)[C]//Data Science and Advanced Analytics (DSAA), 2016 IEEE International Conference on. IEEE, 2016: 41-50.
- 【Transfer Learning】Joy T T, Rana S, Gupta S K, et al. [Flexible transfer learning framework for bayesian optimisation](https://link.springer.com/chapter/10.1007/978-3-319-31753-3_9)[C]//Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, Cham, 2016: 102-114.

#### 2017
- 【Architecture Search】【Evolutionary Algorithms】Real E, Moore S, Selle A, et al. [Large-scale evolution of image classifiers](https://arxiv.org/abs/1703.01041)[J]. arXiv preprint arXiv:1703.01041, 2017.
- 【Lipschitz Functions】Malherbe C, Vayatis N. [Global optimization of Lipschitz functions](https://arxiv.org/pdf/1703.02628.pdf)[J]. arXiv preprint arXiv:1703.02628, 2017.
- 【Architecture Search】Hazan E, Klivans A, Yuan Y. [Hyperparameter Optimization: A Spectral Approach](https://arxiv.org/abs/1706.00764
)[J]. arXiv preprint arXiv:1706.00764, 2017.
- 【Architecture Search】Cai H, Chen T, Zhang W, et al. [Efficient Architecture Search by Network Transformation](https://arxiv.org/abs/1707.04873)[J].  arXiv preprint arXiv:1707.04873, 2017. (Cai et al. 2017)
- 【Architecture Search】Zoph B, Vasudevan V, Shlens J, et al. [Learning transferable architectures for scalable image recognition](https://arxiv.org/abs/1707.07012)[J]. arXiv preprint arXiv:1707.07012, 2017.
- 【Articles】【Meta Learning】[Learning to learn](http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/)
- 【Articles】【Meta Learning】[Why Meta-learning is Crucial for Further Advances of Artificial Intelligence?](https://chatbotslife.com/why-meta-learning-is-crucial-for-further-advances-of-artificial-intelligence-c2df55959adf)
- 【Architecture Search】Brock A, Lim T, Ritchie J M, et al. [SMASH: one-shot model architecture search through hypernetworks](https://arxiv.org/abs/1708.05344)[J]. arXiv preprint arXiv:1708.05344, 2017.
- 【Architecture Search】Bello I, Zoph B, Vasudevan V, et al. [Neural optimizer search with reinforcement learning](https://arxiv.org/abs/1709.07417)[J]. arXiv preprint arXiv:1709.07417, 2017.
- 【Architecture Search】Liu H, Simonyan K, Vinyals O, et al. [Hierarchical representations for efficient architecture search](https://arxiv.org/abs/1711.00436)[J]. arXiv preprint arXiv:1711.00436, 2017.
- 【Architecture Search】Elsken T, Metzen J H, Hutter F. [Simple and efficient architecture search for convolutional neural networks](https://arxiv.org/abs/1711.04528)[J]. arXiv preprint arXiv:1711.04528, 2017.
- 【Architecture Search】Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, Koray Kavukcuoglu. [Population Based Training of Neural Networks](https://arxiv.org/abs/1711.09846)[J]. arXiv preprint arXiv:1711.09846, 2017.
- 【Architecture Search】Liu C, Zoph B, Shlens J, et al. [Progressive neural architecture search](https://arxiv.org/abs/1712.00559)[J]. arXiv preprint arXiv:1712.00559, 2017.
- 【Architecture Search】Wistuba M. [Finding Competitive Network Architectures Within a Day Using UCT](https://arxiv.org/abs/1712.07420)[J]. arXiv preprint arXiv:1712.07420, 2017.
- 【Particle Swarm Optimization】Lorenzo P R, Nalepa J, Kawulok M, et al. [Particle swarm optimization for hyper-parameter selection in deep neural networks](https://dl.acm.org/citation.cfm?id=3071208)[C]//Proceedings of the Genetic and Evolutionary Computation Conference. ACM, 2017: 481-488.
- 【Miscellaneous】Martin Wistuba, et al. [Automatic Frankensteining: Creating Complex Ensembles Autonomously](http://epubs.siam.org/doi/pdf/10.1137/1.9781611974973.83)

#### 2018
- 【Architecture Search】Real E, Aggarwal A, Huang Y, et al. [Regularized Evolution for Image Classifier Architecture Search](https://arxiv.org/abs/1802.01548)[J]. arXiv preprint arXiv:1802.01548, 2018.
- 【Architecture Search】Pham H, Guan M Y, Zoph B, et al. [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/abs/1802.03268)[J]. arXiv preprint arXiv:1802.03268, 2018.
- 【Architecture Search】Kandasamy K, Neiswanger W, Schneider J, et al. [Neural Architecture Search with Bayesian Optimisation and Optimal Transport](https://arxiv.org/abs/1802.07191)[J]. arXiv preprint arXiv:1802.07191, 2018.
- 【Evolutionary Algorithms】Chen B, Wu H, Mo W, et al. [Autostacker: A Compositional Evolutionary Learning System](https://arxiv.org/pdf/1803.00684.pdf)[J]. arXiv preprint arXiv:1803.00684, 2018.
- 【Make it more efficient】Wong C, Houlsby N, Lu Y, et al. [Transfer Automatic Machine Learning](https://arxiv.org/abs/1803.02780)[J]. arXiv preprint arXiv:1803.02780, 2018.
- 【Architecture Search】Kamath P, Singh A, Dutta D. [Neural Architecture Construction using EnvelopeNets](https://arxiv.org/abs/1803.06744)[J]. arXiv preprint arXiv:1803.06744, 2018.
- 【Make it more efficient】Bennani-Smires K, Musat C, Hossmann A, et al. [GitGraph-from Computational Subgraphs to Smaller Architecture Search Spaces](https://openreview.net/pdf?id=rkiO1_1Pz)[J]. 2018.
- 【Multi-Objective NAS】Dong J D, Cheng A C, Juan D C, et al. [PPP-Net: Platform-aware Progressive Search for Pareto-optimal Neural Architectures](https://openreview.net/pdf?id=B1NT3TAIM)[J]. 2018.
- 【Miscellaneous】Bowen Baker, et al. Baker B, Gupta O, Raskar R, et al. [Accelerating Neural Architecture Search using Performance Prediction](https://openreview.net/pdf?id=BJypUGZ0Z)[J]. 2018.
